{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "Кудрявцев Николай Михайлович. КИ19-09Б, 031940750\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(float) / 255.0\n",
    "    \n",
    "    # Вычтем среднее арифметическое\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Добавим канал с единицами как постоянное смещение\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Разделим train на train и val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Реализовали softmax и cross-entropy для единичной выборки\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Убедились, что с большими числами также работает\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Реализовали функцию, объединяющую softmax и cross entropy и вычисляющую градиент\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "# Тест batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float64)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int32)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Тест batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float64)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int32)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Снова убедимся, что нормирование работает для больших чисел в каждом батче.\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float64)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float64)\n",
    "target_index = np.ones(batch_size, dtype=np.int32)\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Реализовали функцию l2_regularization, вычисляющую ошибку для L2 регуляризации\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 690.883617\n",
      "Epoch 1, loss: 688.211508\n",
      "Epoch 2, loss: 685.841336\n",
      "Epoch 3, loss: 683.710981\n",
      "Epoch 4, loss: 681.806399\n",
      "Epoch 5, loss: 680.090137\n",
      "Epoch 6, loss: 678.596557\n",
      "Epoch 7, loss: 677.321790\n",
      "Epoch 8, loss: 676.472948\n",
      "Epoch 9, loss: 676.264172\n"
     ]
    }
   ],
   "source": [
    "# Реализовали функцию LinearSoftmaxClassifier.fit\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d23f44dd00>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Посмотрим историю изменения ошибки!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 5 1 1 1 1 3 1 1 1 1 3 2 5 2 1 1 1 2 1 1 1 1 1 2 2 1 3 2 3 3 1 1 1 1 1\n",
      " 1 2 1 2 2 2 2 3 2 1 1 2 1 1 1 0 5 1 2 2 3 2 1 2 1 3 1 1 2 1 1 1 1 2 1 1 1\n",
      " 1 1 3 1 3 2 1 3 2 7 2 3 1 1 1 1 1 3 1 1 1 1 1 3 3 3 1 1 1 2 1 2 1 2 2 3 2\n",
      " 2 3 2 1 3 2 7 1 1 1 3 2 1 2 3 1 1 1 3 1 1 2 2 2 1 1 1 2 1 1 2 2 2 2 1 1 5\n",
      " 1 1 1 1 1 1 1 2 5 1 1 2 1 1 1 1 3 2 1 6 1 2 3 1 1 1 1 5 1 1 2 2 1 1 1 1 1\n",
      " 1 2 2 1 1 2 1 1 5 1 1 2 1 1 1 3 1 3 1 2 1 2 2 2 1 1 2 1 1 5 2 1 1 1 3 1 1\n",
      " 1 2 1 3 1 1 1 3 1 1 1 1 1 3 1 1 1 3 1 1 1 3 1 1 1 2 1 1 1 1 2 1 3 1 1 2 1\n",
      " 1 1 3 3 1 1 1 2 1 3 1 1 1 1 2 1 2 1 3 1 1 1 3 5 1 2 2 1 1 1 1 2 3 1 2 7 1\n",
      " 1 1 1 1 1 1 2 1 1 1 1 1 1 3 1 1 1 1 2 3 1 6 2 2 5 2 5 2 1 1 1 1 3 6 1 1 1\n",
      " 1 1 1 2 1 3 5 2 5 1 2 1 1 1 1 3 1 2 1 2 2 1 3 2 1 1 2 5 1 1 1 1 7 1 1 3 3\n",
      " 2 1 2 1 2 1 1 3 1 1 3 1 2 1 1 3 1 1 2 1 1 1 1 5 3 2 1 1 3 1 1 1 1 2 1 1 2\n",
      " 1 3 1 3 1 7 1 1 1 1 6 1 1 1 1 2 1 1 1 3 1 1 2 7 7 2 1 1 2 7 1 6 1 1 1 5 1\n",
      " 1 2 1 1 1 1 1 6 3 3 1 2 3 1 3 1 1 1 1 1 2 1 1 3 3 1 2 2 1 1 2 3 1 2 2 1 2\n",
      " 1 1 2 5 1 1 1 1 1 1 2 3 1 1 1 2 3 1 2 2 3 1 1 2 3 1 1 1 1 1 5 1 1 1 1 1 2\n",
      " 3 1 1 1 2 1 5 1 1 1 1 2 1 1 2 2 2 5 2 1 2 3 3 1 1 2 2 1 1 2 2 2 2 3 3 1 3\n",
      " 1 2 1 1 1 2 1 1 1 3 3 1 3 1 2 3 1 1 3 1 2 1 3 1 2 1 1 1 1 1 1 1 1 1 3 1 3\n",
      " 2 1 1 1 3 2 1 1 1 1 2 1 1 1 2 1 1 2 1 1 1 1 2 1 1 1 1 1 1 2 2 2 2 3 1 3 1\n",
      " 1 1 1 2 2 2 1 1 2 6 1 1 1 3 1 2 2 2 1 1 1 3 3 1 2 1 1 1 3 2 1 1 2 2 2 3 3\n",
      " 2 1 1 1 5 1 2 1 3 1 3 1 2 1 1 3 1 1 3 2 1 2 3 1 1 1 1 2 3 1 1 1 1 1 1 2 1\n",
      " 1 1 1 2 3 5 2 1 2 2 1 1 3 2 1 3 1 1 3 2 1 1 3 2 2 3 1 1 1 1 3 1 6 1 1 1 2\n",
      " 1 1 3 2 2 1 2 2 3 7 2 1 2 1 1 1 7 1 1 2 2 1 3 2 2 1 1 2 1 1 2 2 1 3 1 1 1\n",
      " 2 1 3 3 5 3 2 2 3 1 2 1 1 1 1 1 3 3 1 5 0 1 3 1 1 2 1 3 1 3 2 2 1 1 2 1 1\n",
      " 1 3 2 1 1 2 1 1 1 2 1 1 2 1 1 5 1 2 2 3 2 1 1 1 3 2 3 1 1 1 1 1 1 1 1 1 1\n",
      " 3 2 0 1 1 3 2 1 1 2 1 1 1 2 1 1 1 2 1 1 2 2 1 1 1 1 2 1 3 1 7 1 2 1 7 2 2\n",
      " 3 1 2 2 3 3 1 1 1 2 2 1 3 1 2 1 2 2 3 2 2 1 1 3 1 2 1 1 5 1 2 3 1 1 1 2 2\n",
      " 8 1 1 5 1 0 0 1 1 1 4 1 1 1 2 2 1 2 1 1 1 5 1 3 1 1 1 1 2 1 1 3 1 2 1 3 1\n",
      " 6 1 2 1 1 1 2 2 7 1 2 2 3 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 2 2 1 0 1 1 3 2\n",
      " 1]\n",
      "[2 9 8 4 6 5 3 3 3 1 1 7 5 2 8 8 3 3 2 3 3 7 3 1 4 8 1 7 0 1 8 8 4 6 1 1 0\n",
      " 6 1 5 1 1 1 5 1 3 5 2 8 2 7 1 7 4 3 3 6 5 7 3 2 3 1 5 1 2 3 4 2 3 3 2 1 8\n",
      " 3 1 1 5 5 0 9 6 2 0 9 3 1 1 3 4 1 1 3 2 2 4 7 6 6 1 9 7 3 7 1 4 9 0 1 1 6\n",
      " 7 1 7 6 5 1 5 1 1 4 4 1 3 6 5 4 1 1 5 2 1 2 1 7 7 5 1 1 1 1 9 7 3 2 8 6 2\n",
      " 1 2 2 9 2 1 6 0 4 0 1 3 0 9 8 3 3 1 2 8 2 1 3 6 2 4 1 2 4 8 5 2 1 9 5 3 1\n",
      " 4 8 5 1 7 7 3 7 4 1 1 4 5 5 2 5 1 1 2 3 3 5 1 9 7 2 4 1 7 4 3 2 2 6 2 0 6\n",
      " 1 6 5 5 8 2 5 1 1 1 2 0 6 1 5 6 1 1 8 8 3 4 5 2 7 1 8 1 2 9 3 9 1 8 0 9 3\n",
      " 7 1 9 2 5 7 7 1 3 2 1 4 2 0 0 3 4 4 3 6 8 3 3 1 5 7 0 9 1 1 1 9 3 7 1 2 8\n",
      " 1 1 2 7 8 1 4 5 3 3 7 9 1 9 7 8 1 3 2 1 6 4 3 2 1 1 6 1 1 1 1 1 9 6 2 2 2\n",
      " 2 7 4 2 7 9 5 2 3 1 9 4 0 1 8 7 5 7 2 7 3 3 6 2 2 6 6 1 6 5 3 1 1 2 0 3 8\n",
      " 4 9 3 4 7 8 0 5 4 7 6 1 3 4 5 9 1 9 8 2 5 0 3 6 1 2 3 1 6 2 4 3 9 3 0 9 1\n",
      " 6 4 6 3 4 8 1 9 6 9 6 5 1 6 3 1 3 0 5 2 6 9 3 8 4 2 1 7 7 0 7 5 2 4 3 6 1\n",
      " 2 8 1 5 4 7 1 0 3 3 1 7 4 3 1 7 1 5 0 3 5 4 2 2 4 0 3 1 3 2 7 3 6 6 1 1 3\n",
      " 1 3 4 2 5 3 1 2 4 1 1 4 5 1 9 9 2 6 4 2 5 6 3 2 3 4 5 0 2 2 1 2 3 4 4 1 9\n",
      " 1 1 3 6 2 2 8 6 1 4 1 3 3 1 2 2 5 1 3 3 5 1 3 6 3 5 2 2 3 4 4 7 7 4 5 3 4\n",
      " 2 1 7 7 0 7 9 9 1 5 8 7 6 4 9 4 1 1 6 2 6 1 5 8 2 6 5 2 2 6 5 6 6 3 0 1 3\n",
      " 7 5 0 4 2 3 1 8 5 1 3 3 5 9 1 3 2 9 5 4 8 1 8 2 8 1 9 2 1 2 1 9 6 4 4 1 0\n",
      " 2 2 4 1 1 1 8 6 3 7 2 1 1 8 4 6 2 4 0 7 3 1 8 8 1 2 7 2 3 1 3 7 1 4 2 5 6\n",
      " 4 7 9 8 6 2 2 8 2 3 1 5 0 8 8 1 3 1 5 1 1 0 8 6 3 3 0 2 6 4 6 5 3 7 1 2 4\n",
      " 6 5 6 7 5 6 2 3 3 0 9 0 3 8 3 6 1 7 4 6 3 9 3 2 4 3 3 0 5 3 2 2 4 1 3 6 2\n",
      " 1 9 6 3 1 7 3 2 4 4 7 2 2 6 3 5 6 1 5 5 7 2 1 3 9 1 5 2 6 9 5 8 4 1 7 1 5\n",
      " 2 4 4 3 1 9 3 9 8 3 1 3 0 8 4 2 3 8 3 4 1 9 1 3 3 4 4 1 7 5 0 3 7 4 5 9 0\n",
      " 5 4 2 4 3 2 3 1 7 5 2 1 0 7 6 4 6 7 5 7 7 2 1 2 2 1 0 3 9 1 5 1 4 1 5 5 3\n",
      " 0 1 1 0 3 1 1 9 2 1 5 3 2 7 1 1 3 3 1 1 4 3 3 7 9 7 2 7 5 3 8 6 3 0 6 2 8\n",
      " 6 4 3 1 8 1 0 1 1 4 1 1 0 3 3 6 4 8 5 0 4 8 2 4 1 2 2 1 6 1 3 9 1 5 2 1 3\n",
      " 5 7 1 8 6 1 5 5 0 3 4 2 2 2 1 9 5 1 9 3 8 2 2 1 9 2 6 4 1 8 6 2 3 1 5 2 9\n",
      " 8 1 3 7 1 2 9 2 4 2 2 0 8 9 2 5 1 2 1 9 7 4 3 1 1 2 3 2 8 4 7 3 1 5 2 5 3\n",
      " 3]\n",
      "Accuracy:  0.186\n",
      "Epoch 0, loss: 678.288581\n",
      "Epoch 1, loss: 675.760484\n",
      "Epoch 2, loss: 674.224831\n",
      "Epoch 3, loss: 673.279360\n",
      "Epoch 4, loss: 672.683281\n",
      "Epoch 5, loss: 672.292405\n",
      "Epoch 6, loss: 672.021068\n",
      "Epoch 7, loss: 671.818937\n",
      "Epoch 8, loss: 671.656759\n",
      "Epoch 9, loss: 671.517694\n",
      "Epoch 10, loss: 671.392106\n",
      "Epoch 11, loss: 671.274509\n",
      "Epoch 12, loss: 671.161785\n",
      "Epoch 13, loss: 671.052161\n",
      "Epoch 14, loss: 670.944624\n",
      "Epoch 15, loss: 670.838597\n",
      "Epoch 16, loss: 670.733747\n",
      "Epoch 17, loss: 670.629878\n",
      "Epoch 18, loss: 670.526877\n",
      "Epoch 19, loss: 670.424674\n",
      "Epoch 20, loss: 670.323226\n",
      "Epoch 21, loss: 670.222505\n",
      "Epoch 22, loss: 670.122493\n",
      "Epoch 23, loss: 670.023175\n",
      "Epoch 24, loss: 669.924542\n",
      "Epoch 25, loss: 669.826585\n",
      "Epoch 26, loss: 669.729296\n",
      "Epoch 27, loss: 669.632669\n",
      "Epoch 28, loss: 669.536697\n",
      "Epoch 29, loss: 669.441375\n",
      "Epoch 30, loss: 669.346696\n",
      "Epoch 31, loss: 669.252655\n",
      "Epoch 32, loss: 669.159247\n",
      "Epoch 33, loss: 669.066467\n",
      "Epoch 34, loss: 668.974308\n",
      "Epoch 35, loss: 668.882766\n",
      "Epoch 36, loss: 668.791836\n",
      "Epoch 37, loss: 668.701512\n",
      "Epoch 38, loss: 668.611790\n",
      "Epoch 39, loss: 668.522664\n",
      "Epoch 40, loss: 668.434131\n",
      "Epoch 41, loss: 668.346183\n",
      "Epoch 42, loss: 668.258818\n",
      "Epoch 43, loss: 668.172030\n",
      "Epoch 44, loss: 668.085815\n",
      "Epoch 45, loss: 668.000167\n",
      "Epoch 46, loss: 667.915082\n",
      "Epoch 47, loss: 667.830556\n",
      "Epoch 48, loss: 667.746584\n",
      "Epoch 49, loss: 667.663161\n",
      "Epoch 50, loss: 667.580284\n",
      "Epoch 51, loss: 667.497946\n",
      "Epoch 52, loss: 667.416146\n",
      "Epoch 53, loss: 667.334877\n",
      "Epoch 54, loss: 667.254135\n",
      "Epoch 55, loss: 667.173917\n",
      "Epoch 56, loss: 667.094218\n",
      "Epoch 57, loss: 667.015034\n",
      "Epoch 58, loss: 666.936361\n",
      "Epoch 59, loss: 666.858195\n",
      "Epoch 60, loss: 666.780531\n",
      "Epoch 61, loss: 666.703365\n",
      "Epoch 62, loss: 666.626695\n",
      "Epoch 63, loss: 666.550515\n",
      "Epoch 64, loss: 666.474822\n",
      "Epoch 65, loss: 666.399611\n",
      "Epoch 66, loss: 666.324880\n",
      "Epoch 67, loss: 666.250624\n",
      "Epoch 68, loss: 666.176839\n",
      "Epoch 69, loss: 666.103522\n",
      "Epoch 70, loss: 666.030670\n",
      "Epoch 71, loss: 665.958277\n",
      "Epoch 72, loss: 665.886341\n",
      "Epoch 73, loss: 665.814859\n",
      "Epoch 74, loss: 665.743826\n",
      "Epoch 75, loss: 665.673238\n",
      "Epoch 76, loss: 665.603094\n",
      "Epoch 77, loss: 665.533388\n",
      "Epoch 78, loss: 665.464118\n",
      "Epoch 79, loss: 665.395280\n",
      "Epoch 80, loss: 665.326871\n",
      "Epoch 81, loss: 665.258888\n",
      "Epoch 82, loss: 665.191327\n",
      "Epoch 83, loss: 665.124184\n",
      "Epoch 84, loss: 665.057457\n",
      "Epoch 85, loss: 664.991142\n",
      "Epoch 86, loss: 664.925237\n",
      "Epoch 87, loss: 664.859737\n",
      "Epoch 88, loss: 664.794641\n",
      "Epoch 89, loss: 664.729944\n",
      "Epoch 90, loss: 664.665643\n",
      "Epoch 91, loss: 664.601737\n",
      "Epoch 92, loss: 664.538221\n",
      "Epoch 93, loss: 664.475092\n",
      "Epoch 94, loss: 664.412348\n",
      "Epoch 95, loss: 664.349985\n",
      "Epoch 96, loss: 664.288002\n",
      "Epoch 97, loss: 664.226394\n",
      "Epoch 98, loss: 664.165159\n",
      "Epoch 99, loss: 664.104294\n",
      "Accuracy after training for 100 epochs:  0.226\n"
     ]
    }
   ],
   "source": [
    "# TODO: Реализовать функцию вычисления точности и посмотреть точность обучения\n",
    "pred = classifier.predict(val_X)\n",
    "print(pred)\n",
    "print(val_y)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Теперь возьмем больше эпох, меньшую скорость обучения и посмотрим как изменится точность\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-4, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметров.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 735.177058\n",
      "Epoch 1, loss: 721.690580\n",
      "Epoch 2, loss: 739.277113\n",
      "Epoch 3, loss: 730.188472\n",
      "Epoch 4, loss: 753.539611\n",
      "Epoch 5, loss: 735.940524\n",
      "Epoch 6, loss: 752.079509\n",
      "Epoch 7, loss: 732.643130\n",
      "Epoch 8, loss: 746.760634\n",
      "Epoch 9, loss: 728.543049\n",
      "Epoch 10, loss: 741.597774\n",
      "Epoch 11, loss: 725.922472\n",
      "Epoch 12, loss: 738.066165\n",
      "Epoch 13, loss: 727.435826\n",
      "Epoch 14, loss: 739.657788\n",
      "Epoch 15, loss: 740.906360\n",
      "Epoch 16, loss: 755.379842\n",
      "Epoch 17, loss: 771.554526\n",
      "Epoch 18, loss: 770.423610\n",
      "Epoch 19, loss: 775.359758\n",
      "Epoch 20, loss: 751.134110\n",
      "Epoch 21, loss: 760.763719\n",
      "Epoch 22, loss: 735.442017\n",
      "Epoch 23, loss: 746.550242\n",
      "Epoch 24, loss: 717.737459\n",
      "Epoch 25, loss: 726.330702\n",
      "Epoch 26, loss: 697.265927\n",
      "Epoch 27, loss: 704.238051\n",
      "Epoch 28, loss: 675.828450\n",
      "Epoch 29, loss: 679.813099\n",
      "Epoch 30, loss: 652.719801\n",
      "Epoch 31, loss: 651.442783\n",
      "Epoch 32, loss: 628.277750\n",
      "Epoch 33, loss: 622.677448\n",
      "Epoch 34, loss: 615.156855\n",
      "Epoch 35, loss: 616.447448\n",
      "Epoch 36, loss: 625.411206\n",
      "Epoch 37, loss: 640.226065\n",
      "Epoch 38, loss: 693.384817\n",
      "Epoch 39, loss: 751.690877\n",
      "Epoch 40, loss: 815.094348\n",
      "Epoch 41, loss: 801.970670\n",
      "Epoch 42, loss: 833.472554\n",
      "Epoch 43, loss: 846.153016\n",
      "Epoch 44, loss: 879.610127\n",
      "Epoch 45, loss: 878.340844\n",
      "Epoch 46, loss: 868.140516\n",
      "Epoch 47, loss: 856.009116\n",
      "Epoch 48, loss: 841.141382\n",
      "Epoch 49, loss: 832.762771\n",
      "Epoch 50, loss: 815.205138\n",
      "Epoch 51, loss: 810.094809\n",
      "Epoch 52, loss: 788.396935\n",
      "Epoch 53, loss: 788.231489\n",
      "Epoch 54, loss: 764.257703\n",
      "Epoch 55, loss: 778.278033\n",
      "Epoch 56, loss: 761.444881\n",
      "Epoch 57, loss: 781.786347\n",
      "Epoch 58, loss: 770.530128\n",
      "Epoch 59, loss: 798.066246\n",
      "Epoch 60, loss: 825.304140\n",
      "Epoch 61, loss: 864.173937\n",
      "Epoch 62, loss: 877.167567\n",
      "Epoch 63, loss: 846.524700\n",
      "Epoch 64, loss: 844.042522\n",
      "Epoch 65, loss: 810.820901\n",
      "Epoch 66, loss: 812.232351\n",
      "Epoch 67, loss: 778.183949\n",
      "Epoch 68, loss: 783.559009\n",
      "Epoch 69, loss: 749.271395\n",
      "Epoch 70, loss: 762.070012\n",
      "Epoch 71, loss: 730.277907\n",
      "Epoch 72, loss: 745.972565\n",
      "Epoch 73, loss: 715.055794\n",
      "Epoch 74, loss: 726.855507\n",
      "Epoch 75, loss: 697.867035\n",
      "Epoch 76, loss: 705.669295\n",
      "Epoch 77, loss: 678.888263\n",
      "Epoch 78, loss: 682.115897\n",
      "Epoch 79, loss: 657.515321\n",
      "Epoch 80, loss: 654.838858\n",
      "Epoch 81, loss: 633.878903\n",
      "Epoch 82, loss: 626.559848\n",
      "Epoch 83, loss: 619.314310\n",
      "Epoch 84, loss: 618.805535\n",
      "Epoch 85, loss: 624.547005\n",
      "Epoch 86, loss: 627.944296\n",
      "Epoch 87, loss: 647.967272\n",
      "Epoch 88, loss: 669.151427\n",
      "Epoch 89, loss: 736.856969\n",
      "Epoch 90, loss: 756.969254\n",
      "Epoch 91, loss: 803.230612\n",
      "Epoch 92, loss: 791.655807\n",
      "Epoch 93, loss: 844.642103\n",
      "Epoch 94, loss: 856.805221\n",
      "Epoch 95, loss: 852.938586\n",
      "Epoch 96, loss: 834.310949\n",
      "Epoch 97, loss: 809.217427\n",
      "Epoch 98, loss: 803.880509\n",
      "Epoch 99, loss: 785.649732\n",
      "0.001 0.0001 - 0.204\n",
      "Epoch 0, loss: 813.097569\n",
      "Epoch 1, loss: 813.252877\n",
      "Epoch 2, loss: 862.696895\n",
      "Epoch 3, loss: 904.588810\n",
      "Epoch 4, loss: 913.518795\n",
      "Epoch 5, loss: 904.616166\n",
      "Epoch 6, loss: 884.753931\n",
      "Epoch 7, loss: 879.717760\n",
      "Epoch 8, loss: 861.597246\n",
      "Epoch 9, loss: 857.691489\n",
      "Epoch 10, loss: 840.881176\n",
      "Epoch 11, loss: 837.217955\n",
      "Epoch 12, loss: 821.723128\n",
      "Epoch 13, loss: 817.847537\n",
      "Epoch 14, loss: 803.834324\n",
      "Epoch 15, loss: 799.360939\n",
      "Epoch 16, loss: 787.113978\n",
      "Epoch 17, loss: 781.547771\n",
      "Epoch 18, loss: 771.499927\n",
      "Epoch 19, loss: 764.051629\n",
      "Epoch 20, loss: 756.886399\n",
      "Epoch 21, loss: 746.249166\n",
      "Epoch 22, loss: 743.404608\n",
      "Epoch 23, loss: 728.413964\n",
      "Epoch 24, loss: 735.452793\n",
      "Epoch 25, loss: 722.244301\n",
      "Epoch 26, loss: 746.078614\n",
      "Epoch 27, loss: 732.069901\n",
      "Epoch 28, loss: 750.758371\n",
      "Epoch 29, loss: 732.737189\n",
      "Epoch 30, loss: 747.985322\n",
      "Epoch 31, loss: 732.132624\n",
      "Epoch 32, loss: 746.304281\n",
      "Epoch 33, loss: 737.642707\n",
      "Epoch 34, loss: 753.435794\n",
      "Epoch 35, loss: 761.701859\n",
      "Epoch 36, loss: 777.728897\n",
      "Epoch 37, loss: 790.445748\n",
      "Epoch 38, loss: 777.424359\n",
      "Epoch 39, loss: 775.638186\n",
      "Epoch 40, loss: 748.437819\n",
      "Epoch 41, loss: 756.607987\n",
      "Epoch 42, loss: 730.558899\n",
      "Epoch 43, loss: 741.560459\n",
      "Epoch 44, loss: 713.076134\n",
      "Epoch 45, loss: 721.675916\n",
      "Epoch 46, loss: 693.264274\n",
      "Epoch 47, loss: 699.590845\n",
      "Epoch 48, loss: 672.181748\n",
      "Epoch 49, loss: 674.677410\n",
      "Epoch 50, loss: 649.378009\n",
      "Epoch 51, loss: 646.155272\n",
      "Epoch 52, loss: 629.343050\n",
      "Epoch 53, loss: 628.605643\n",
      "Epoch 54, loss: 650.059202\n",
      "Epoch 55, loss: 687.532800\n",
      "Epoch 56, loss: 776.010403\n",
      "Epoch 57, loss: 783.270162\n",
      "Epoch 58, loss: 806.414670\n",
      "Epoch 59, loss: 772.122968\n",
      "Epoch 60, loss: 793.684125\n",
      "Epoch 61, loss: 767.949803\n",
      "Epoch 62, loss: 794.804329\n",
      "Epoch 63, loss: 796.982328\n",
      "Epoch 64, loss: 831.647904\n",
      "Epoch 65, loss: 836.696823\n",
      "Epoch 66, loss: 823.553838\n",
      "Epoch 67, loss: 811.006968\n",
      "Epoch 68, loss: 785.800323\n",
      "Epoch 69, loss: 794.687928\n",
      "Epoch 70, loss: 786.621461\n",
      "Epoch 71, loss: 816.947623\n",
      "Epoch 72, loss: 831.984848\n",
      "Epoch 73, loss: 884.559856\n",
      "Epoch 74, loss: 909.429046\n",
      "Epoch 75, loss: 895.011869\n",
      "Epoch 76, loss: 887.651584\n",
      "Epoch 77, loss: 868.014597\n",
      "Epoch 78, loss: 863.916097\n",
      "Epoch 79, loss: 845.765143\n",
      "Epoch 80, loss: 842.656798\n",
      "Epoch 81, loss: 825.703271\n",
      "Epoch 82, loss: 822.846714\n",
      "Epoch 83, loss: 807.160629\n",
      "Epoch 84, loss: 804.143081\n",
      "Epoch 85, loss: 789.960530\n",
      "Epoch 86, loss: 786.385221\n",
      "Epoch 87, loss: 774.088700\n",
      "Epoch 88, loss: 769.402227\n",
      "Epoch 89, loss: 759.574036\n",
      "Epoch 90, loss: 752.866322\n",
      "Epoch 91, loss: 746.443265\n",
      "Epoch 92, loss: 736.267243\n",
      "Epoch 93, loss: 735.383297\n",
      "Epoch 94, loss: 721.417056\n",
      "Epoch 95, loss: 733.827881\n",
      "Epoch 96, loss: 723.169565\n",
      "Epoch 97, loss: 746.385082\n",
      "Epoch 98, loss: 729.211339\n",
      "Epoch 99, loss: 746.208243\n",
      "0.001 1e-05 - 0.186\n",
      "Epoch 0, loss: 725.611386\n",
      "Epoch 1, loss: 740.730563\n",
      "Epoch 2, loss: 720.028580\n",
      "Epoch 3, loss: 734.528242\n",
      "Epoch 4, loss: 714.320880\n",
      "Epoch 5, loss: 728.508853\n",
      "Epoch 6, loss: 708.913517\n",
      "Epoch 7, loss: 722.922163\n",
      "Epoch 8, loss: 703.972229\n",
      "Epoch 9, loss: 717.859970\n",
      "Epoch 10, loss: 699.568046\n",
      "Epoch 11, loss: 713.343556\n",
      "Epoch 12, loss: 695.712157\n",
      "Epoch 13, loss: 709.365804\n",
      "Epoch 14, loss: 692.391637\n",
      "Epoch 15, loss: 705.922596\n",
      "Epoch 16, loss: 689.597672\n",
      "Epoch 17, loss: 703.023780\n",
      "Epoch 18, loss: 687.334817\n",
      "Epoch 19, loss: 700.689546\n",
      "Epoch 20, loss: 685.619293\n",
      "Epoch 21, loss: 698.943904\n",
      "Epoch 22, loss: 684.478432\n",
      "Epoch 23, loss: 697.813207\n",
      "Epoch 24, loss: 683.963305\n",
      "Epoch 25, loss: 697.339747\n",
      "Epoch 26, loss: 684.201642\n",
      "Epoch 27, loss: 697.645260\n",
      "Epoch 28, loss: 685.595172\n",
      "Epoch 29, loss: 699.211180\n",
      "Epoch 30, loss: 689.686119\n",
      "Epoch 31, loss: 704.386079\n",
      "Epoch 32, loss: 703.829633\n",
      "Epoch 33, loss: 725.883780\n",
      "Epoch 34, loss: 755.600223\n",
      "Epoch 35, loss: 778.709707\n",
      "Epoch 36, loss: 801.854879\n",
      "Epoch 37, loss: 775.962694\n",
      "Epoch 38, loss: 794.047313\n",
      "Epoch 39, loss: 764.590460\n",
      "Epoch 40, loss: 781.293688\n",
      "Epoch 41, loss: 751.543368\n",
      "Epoch 42, loss: 766.450328\n",
      "Epoch 43, loss: 738.798173\n",
      "Epoch 44, loss: 752.202794\n",
      "Epoch 45, loss: 727.582018\n",
      "Epoch 46, loss: 739.314524\n",
      "Epoch 47, loss: 720.156937\n",
      "Epoch 48, loss: 731.328007\n",
      "Epoch 49, loss: 727.292880\n",
      "Epoch 50, loss: 745.125842\n",
      "Epoch 51, loss: 772.668053\n",
      "Epoch 52, loss: 771.658117\n",
      "Epoch 53, loss: 793.622095\n",
      "Epoch 54, loss: 766.778716\n",
      "Epoch 55, loss: 790.759272\n",
      "Epoch 56, loss: 759.789440\n",
      "Epoch 57, loss: 783.548281\n",
      "Epoch 58, loss: 770.235475\n",
      "Epoch 59, loss: 805.983213\n",
      "Epoch 60, loss: 823.624480\n",
      "Epoch 61, loss: 832.828012\n",
      "Epoch 62, loss: 822.539652\n",
      "Epoch 63, loss: 796.907267\n",
      "Epoch 64, loss: 796.565042\n",
      "Epoch 65, loss: 779.538792\n",
      "Epoch 66, loss: 799.534129\n",
      "Epoch 67, loss: 777.235261\n",
      "Epoch 68, loss: 791.870136\n",
      "Epoch 69, loss: 769.315944\n",
      "Epoch 70, loss: 783.404084\n",
      "Epoch 71, loss: 761.171185\n",
      "Epoch 72, loss: 775.057885\n",
      "Epoch 73, loss: 753.093420\n",
      "Epoch 74, loss: 766.866427\n",
      "Epoch 75, loss: 745.091680\n",
      "Epoch 76, loss: 758.930203\n",
      "Epoch 77, loss: 737.347291\n",
      "Epoch 78, loss: 751.540102\n",
      "Epoch 79, loss: 730.298662\n",
      "Epoch 80, loss: 744.994012\n",
      "Epoch 81, loss: 724.327137\n",
      "Epoch 82, loss: 739.327569\n",
      "Epoch 83, loss: 719.496281\n",
      "Epoch 84, loss: 734.476217\n",
      "Epoch 85, loss: 715.910459\n",
      "Epoch 86, loss: 730.739179\n",
      "Epoch 87, loss: 714.505930\n",
      "Epoch 88, loss: 729.613755\n",
      "Epoch 89, loss: 719.434948\n",
      "Epoch 90, loss: 738.029126\n",
      "Epoch 91, loss: 748.121731\n",
      "Epoch 92, loss: 776.277243\n",
      "Epoch 93, loss: 804.092409\n",
      "Epoch 94, loss: 792.498598\n",
      "Epoch 95, loss: 798.218421\n",
      "Epoch 96, loss: 768.043632\n",
      "Epoch 97, loss: 785.348639\n",
      "Epoch 98, loss: 755.671879\n",
      "Epoch 99, loss: 771.726539\n",
      "0.001 1e-06 - 0.18\n",
      "Epoch 0, loss: 741.596397\n",
      "Epoch 1, loss: 692.599446\n",
      "Epoch 2, loss: 659.198476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, loss: 639.081762\n",
      "Epoch 4, loss: 627.935613\n",
      "Epoch 5, loss: 621.835834\n",
      "Epoch 6, loss: 618.369785\n",
      "Epoch 7, loss: 616.295604\n",
      "Epoch 8, loss: 614.991448\n",
      "Epoch 9, loss: 614.133655\n",
      "Epoch 10, loss: 613.544641\n",
      "Epoch 11, loss: 613.122236\n",
      "Epoch 12, loss: 612.805388\n",
      "Epoch 13, loss: 612.556544\n",
      "Epoch 14, loss: 612.352103\n",
      "Epoch 15, loss: 612.176984\n",
      "Epoch 16, loss: 612.021435\n",
      "Epoch 17, loss: 611.879091\n",
      "Epoch 18, loss: 611.745766\n",
      "Epoch 19, loss: 611.618695\n",
      "Epoch 20, loss: 611.496041\n",
      "Epoch 21, loss: 611.376581\n",
      "Epoch 22, loss: 611.259495\n",
      "Epoch 23, loss: 611.144236\n",
      "Epoch 24, loss: 611.030435\n",
      "Epoch 25, loss: 610.917842\n",
      "Epoch 26, loss: 610.806290\n",
      "Epoch 27, loss: 610.695662\n",
      "Epoch 28, loss: 610.585881\n",
      "Epoch 29, loss: 610.476891\n",
      "Epoch 30, loss: 610.368653\n",
      "Epoch 31, loss: 610.261141\n",
      "Epoch 32, loss: 610.154334\n",
      "Epoch 33, loss: 610.048217\n",
      "Epoch 34, loss: 609.942778\n",
      "Epoch 35, loss: 609.838007\n",
      "Epoch 36, loss: 609.733897\n",
      "Epoch 37, loss: 609.630439\n",
      "Epoch 38, loss: 609.527629\n",
      "Epoch 39, loss: 609.425460\n",
      "Epoch 40, loss: 609.323926\n",
      "Epoch 41, loss: 609.223024\n",
      "Epoch 42, loss: 609.122747\n",
      "Epoch 43, loss: 609.023091\n",
      "Epoch 44, loss: 608.924051\n",
      "Epoch 45, loss: 608.825623\n",
      "Epoch 46, loss: 608.727802\n",
      "Epoch 47, loss: 608.630584\n",
      "Epoch 48, loss: 608.533965\n",
      "Epoch 49, loss: 608.437940\n",
      "Epoch 50, loss: 608.342504\n",
      "Epoch 51, loss: 608.247654\n",
      "Epoch 52, loss: 608.153386\n",
      "Epoch 53, loss: 608.059695\n",
      "Epoch 54, loss: 607.966578\n",
      "Epoch 55, loss: 607.874030\n",
      "Epoch 56, loss: 607.782047\n",
      "Epoch 57, loss: 607.690626\n",
      "Epoch 58, loss: 607.599763\n",
      "Epoch 59, loss: 607.509453\n",
      "Epoch 60, loss: 607.419693\n",
      "Epoch 61, loss: 607.330480\n",
      "Epoch 62, loss: 607.241809\n",
      "Epoch 63, loss: 607.153677\n",
      "Epoch 64, loss: 607.066080\n",
      "Epoch 65, loss: 606.979014\n",
      "Epoch 66, loss: 606.892477\n",
      "Epoch 67, loss: 606.806463\n",
      "Epoch 68, loss: 606.720971\n",
      "Epoch 69, loss: 606.635996\n",
      "Epoch 70, loss: 606.551535\n",
      "Epoch 71, loss: 606.467585\n",
      "Epoch 72, loss: 606.384142\n",
      "Epoch 73, loss: 606.301203\n",
      "Epoch 74, loss: 606.218764\n",
      "Epoch 75, loss: 606.136822\n",
      "Epoch 76, loss: 606.055375\n",
      "Epoch 77, loss: 605.974418\n",
      "Epoch 78, loss: 605.893948\n",
      "Epoch 79, loss: 605.813963\n",
      "Epoch 80, loss: 605.734459\n",
      "Epoch 81, loss: 605.655433\n",
      "Epoch 82, loss: 605.576882\n",
      "Epoch 83, loss: 605.498803\n",
      "Epoch 84, loss: 605.421193\n",
      "Epoch 85, loss: 605.344048\n",
      "Epoch 86, loss: 605.267366\n",
      "Epoch 87, loss: 605.191144\n",
      "Epoch 88, loss: 605.115379\n",
      "Epoch 89, loss: 605.040068\n",
      "Epoch 90, loss: 604.965208\n",
      "Epoch 91, loss: 604.890796\n",
      "Epoch 92, loss: 604.816829\n",
      "Epoch 93, loss: 604.743305\n",
      "Epoch 94, loss: 604.670220\n",
      "Epoch 95, loss: 604.597572\n",
      "Epoch 96, loss: 604.525358\n",
      "Epoch 97, loss: 604.453576\n",
      "Epoch 98, loss: 604.382222\n",
      "Epoch 99, loss: 604.311294\n",
      "0.0001 0.0001 - 0.236\n",
      "Epoch 0, loss: 604.240789\n",
      "Epoch 1, loss: 604.170704\n",
      "Epoch 2, loss: 604.101037\n",
      "Epoch 3, loss: 604.031785\n",
      "Epoch 4, loss: 603.962946\n",
      "Epoch 5, loss: 603.894517\n",
      "Epoch 6, loss: 603.826495\n",
      "Epoch 7, loss: 603.758878\n",
      "Epoch 8, loss: 603.691664\n",
      "Epoch 9, loss: 603.624849\n",
      "Epoch 10, loss: 603.558431\n",
      "Epoch 11, loss: 603.492409\n",
      "Epoch 12, loss: 603.426778\n",
      "Epoch 13, loss: 603.361538\n",
      "Epoch 14, loss: 603.296685\n",
      "Epoch 15, loss: 603.232216\n",
      "Epoch 16, loss: 603.168131\n",
      "Epoch 17, loss: 603.104426\n",
      "Epoch 18, loss: 603.041098\n",
      "Epoch 19, loss: 602.978146\n",
      "Epoch 20, loss: 602.915567\n",
      "Epoch 21, loss: 602.853359\n",
      "Epoch 22, loss: 602.791520\n",
      "Epoch 23, loss: 602.730046\n",
      "Epoch 24, loss: 602.668937\n",
      "Epoch 25, loss: 602.608189\n",
      "Epoch 26, loss: 602.547801\n",
      "Epoch 27, loss: 602.487770\n",
      "Epoch 28, loss: 602.428094\n",
      "Epoch 29, loss: 602.368771\n",
      "Epoch 30, loss: 602.309798\n",
      "Epoch 31, loss: 602.251174\n",
      "Epoch 32, loss: 602.192896\n",
      "Epoch 33, loss: 602.134962\n",
      "Epoch 34, loss: 602.077370\n",
      "Epoch 35, loss: 602.020118\n",
      "Epoch 36, loss: 601.963203\n",
      "Epoch 37, loss: 601.906625\n",
      "Epoch 38, loss: 601.850379\n",
      "Epoch 39, loss: 601.794466\n",
      "Epoch 40, loss: 601.738882\n",
      "Epoch 41, loss: 601.683625\n",
      "Epoch 42, loss: 601.628693\n",
      "Epoch 43, loss: 601.574085\n",
      "Epoch 44, loss: 601.519798\n",
      "Epoch 45, loss: 601.465831\n",
      "Epoch 46, loss: 601.412181\n",
      "Epoch 47, loss: 601.358847\n",
      "Epoch 48, loss: 601.305826\n",
      "Epoch 49, loss: 601.253116\n",
      "Epoch 50, loss: 601.200716\n",
      "Epoch 51, loss: 601.148624\n",
      "Epoch 52, loss: 601.096838\n",
      "Epoch 53, loss: 601.045355\n",
      "Epoch 54, loss: 600.994175\n",
      "Epoch 55, loss: 600.943294\n",
      "Epoch 56, loss: 600.892712\n",
      "Epoch 57, loss: 600.842427\n",
      "Epoch 58, loss: 600.792436\n",
      "Epoch 59, loss: 600.742738\n",
      "Epoch 60, loss: 600.693331\n",
      "Epoch 61, loss: 600.644213\n",
      "Epoch 62, loss: 600.595382\n",
      "Epoch 63, loss: 600.546837\n",
      "Epoch 64, loss: 600.498576\n",
      "Epoch 65, loss: 600.450597\n",
      "Epoch 66, loss: 600.402898\n",
      "Epoch 67, loss: 600.355478\n",
      "Epoch 68, loss: 600.308335\n",
      "Epoch 69, loss: 600.261467\n",
      "Epoch 70, loss: 600.214872\n",
      "Epoch 71, loss: 600.168550\n",
      "Epoch 72, loss: 600.122497\n",
      "Epoch 73, loss: 600.076713\n",
      "Epoch 74, loss: 600.031195\n",
      "Epoch 75, loss: 599.985943\n",
      "Epoch 76, loss: 599.940954\n",
      "Epoch 77, loss: 599.896227\n",
      "Epoch 78, loss: 599.851760\n",
      "Epoch 79, loss: 599.807552\n",
      "Epoch 80, loss: 599.763601\n",
      "Epoch 81, loss: 599.719905\n",
      "Epoch 82, loss: 599.676463\n",
      "Epoch 83, loss: 599.633273\n",
      "Epoch 84, loss: 599.590334\n",
      "Epoch 85, loss: 599.547644\n",
      "Epoch 86, loss: 599.505202\n",
      "Epoch 87, loss: 599.463006\n",
      "Epoch 88, loss: 599.421054\n",
      "Epoch 89, loss: 599.379346\n",
      "Epoch 90, loss: 599.337879\n",
      "Epoch 91, loss: 599.296652\n",
      "Epoch 92, loss: 599.255663\n",
      "Epoch 93, loss: 599.214912\n",
      "Epoch 94, loss: 599.174396\n",
      "Epoch 95, loss: 599.134114\n",
      "Epoch 96, loss: 599.094065\n",
      "Epoch 97, loss: 599.054247\n",
      "Epoch 98, loss: 599.014659\n",
      "Epoch 99, loss: 598.975299\n",
      "0.0001 1e-05 - 0.241\n",
      "Epoch 0, loss: 598.936166\n",
      "Epoch 1, loss: 598.897259\n",
      "Epoch 2, loss: 598.858576\n",
      "Epoch 3, loss: 598.820115\n",
      "Epoch 4, loss: 598.781876\n",
      "Epoch 5, loss: 598.743857\n",
      "Epoch 6, loss: 598.706057\n",
      "Epoch 7, loss: 598.668474\n",
      "Epoch 8, loss: 598.631107\n",
      "Epoch 9, loss: 598.593954\n",
      "Epoch 10, loss: 598.557015\n",
      "Epoch 11, loss: 598.520287\n",
      "Epoch 12, loss: 598.483770\n",
      "Epoch 13, loss: 598.447463\n",
      "Epoch 14, loss: 598.411363\n",
      "Epoch 15, loss: 598.375470\n",
      "Epoch 16, loss: 598.339783\n",
      "Epoch 17, loss: 598.304299\n",
      "Epoch 18, loss: 598.269019\n",
      "Epoch 19, loss: 598.233940\n",
      "Epoch 20, loss: 598.199062\n",
      "Epoch 21, loss: 598.164382\n",
      "Epoch 22, loss: 598.129901\n",
      "Epoch 23, loss: 598.095616\n",
      "Epoch 24, loss: 598.061526\n",
      "Epoch 25, loss: 598.027631\n",
      "Epoch 26, loss: 597.993929\n",
      "Epoch 27, loss: 597.960419\n",
      "Epoch 28, loss: 597.927099\n",
      "Epoch 29, loss: 597.893968\n",
      "Epoch 30, loss: 597.861026\n",
      "Epoch 31, loss: 597.828271\n",
      "Epoch 32, loss: 597.795702\n",
      "Epoch 33, loss: 597.763317\n",
      "Epoch 34, loss: 597.731116\n",
      "Epoch 35, loss: 597.699098\n",
      "Epoch 36, loss: 597.667260\n",
      "Epoch 37, loss: 597.635603\n",
      "Epoch 38, loss: 597.604126\n",
      "Epoch 39, loss: 597.572826\n",
      "Epoch 40, loss: 597.541702\n",
      "Epoch 41, loss: 597.510755\n",
      "Epoch 42, loss: 597.479982\n",
      "Epoch 43, loss: 597.449383\n",
      "Epoch 44, loss: 597.418956\n",
      "Epoch 45, loss: 597.388701\n",
      "Epoch 46, loss: 597.358616\n",
      "Epoch 47, loss: 597.328700\n",
      "Epoch 48, loss: 597.298952\n",
      "Epoch 49, loss: 597.269372\n",
      "Epoch 50, loss: 597.239958\n",
      "Epoch 51, loss: 597.210709\n",
      "Epoch 52, loss: 597.181623\n",
      "Epoch 53, loss: 597.152701\n",
      "Epoch 54, loss: 597.123941\n",
      "Epoch 55, loss: 597.095342\n",
      "Epoch 56, loss: 597.066903\n",
      "Epoch 57, loss: 597.038623\n",
      "Epoch 58, loss: 597.010501\n",
      "Epoch 59, loss: 596.982536\n",
      "Epoch 60, loss: 596.954727\n",
      "Epoch 61, loss: 596.927073\n",
      "Epoch 62, loss: 596.899573\n",
      "Epoch 63, loss: 596.872227\n",
      "Epoch 64, loss: 596.845032\n",
      "Epoch 65, loss: 596.817989\n",
      "Epoch 66, loss: 596.791096\n",
      "Epoch 67, loss: 596.764353\n",
      "Epoch 68, loss: 596.737758\n",
      "Epoch 69, loss: 596.711310\n",
      "Epoch 70, loss: 596.685009\n",
      "Epoch 71, loss: 596.658854\n",
      "Epoch 72, loss: 596.632843\n",
      "Epoch 73, loss: 596.606976\n",
      "Epoch 74, loss: 596.581253\n",
      "Epoch 75, loss: 596.555671\n",
      "Epoch 76, loss: 596.530231\n",
      "Epoch 77, loss: 596.504930\n",
      "Epoch 78, loss: 596.479770\n",
      "Epoch 79, loss: 596.454748\n",
      "Epoch 80, loss: 596.429863\n",
      "Epoch 81, loss: 596.405116\n",
      "Epoch 82, loss: 596.380504\n",
      "Epoch 83, loss: 596.356027\n",
      "Epoch 84, loss: 596.331685\n",
      "Epoch 85, loss: 596.307477\n",
      "Epoch 86, loss: 596.283400\n",
      "Epoch 87, loss: 596.259456\n",
      "Epoch 88, loss: 596.235643\n",
      "Epoch 89, loss: 596.211960\n",
      "Epoch 90, loss: 596.188406\n",
      "Epoch 91, loss: 596.164981\n",
      "Epoch 92, loss: 596.141683\n",
      "Epoch 93, loss: 596.118513\n",
      "Epoch 94, loss: 596.095468\n",
      "Epoch 95, loss: 596.072549\n",
      "Epoch 96, loss: 596.049755\n",
      "Epoch 97, loss: 596.027084\n",
      "Epoch 98, loss: 596.004536\n",
      "Epoch 99, loss: 595.982111\n",
      "0.0001 1e-06 - 0.252\n",
      "Epoch 0, loss: 595.959807\n",
      "Epoch 1, loss: 595.957586\n",
      "Epoch 2, loss: 595.955367\n",
      "Epoch 3, loss: 595.953148\n",
      "Epoch 4, loss: 595.950931\n",
      "Epoch 5, loss: 595.948714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, loss: 595.946500\n",
      "Epoch 7, loss: 595.944286\n",
      "Epoch 8, loss: 595.942073\n",
      "Epoch 9, loss: 595.939862\n",
      "Epoch 10, loss: 595.937652\n",
      "Epoch 11, loss: 595.935443\n",
      "Epoch 12, loss: 595.933235\n",
      "Epoch 13, loss: 595.931028\n",
      "Epoch 14, loss: 595.928823\n",
      "Epoch 15, loss: 595.926619\n",
      "Epoch 16, loss: 595.924416\n",
      "Epoch 17, loss: 595.922214\n",
      "Epoch 18, loss: 595.920013\n",
      "Epoch 19, loss: 595.917814\n",
      "Epoch 20, loss: 595.915616\n",
      "Epoch 21, loss: 595.913418\n",
      "Epoch 22, loss: 595.911223\n",
      "Epoch 23, loss: 595.909028\n",
      "Epoch 24, loss: 595.906834\n",
      "Epoch 25, loss: 595.904642\n",
      "Epoch 26, loss: 595.902451\n",
      "Epoch 27, loss: 595.900261\n",
      "Epoch 28, loss: 595.898072\n",
      "Epoch 29, loss: 595.895885\n",
      "Epoch 30, loss: 595.893698\n",
      "Epoch 31, loss: 595.891513\n",
      "Epoch 32, loss: 595.889329\n",
      "Epoch 33, loss: 595.887146\n",
      "Epoch 34, loss: 595.884965\n",
      "Epoch 35, loss: 595.882784\n",
      "Epoch 36, loss: 595.880605\n",
      "Epoch 37, loss: 595.878427\n",
      "Epoch 38, loss: 595.876250\n",
      "Epoch 39, loss: 595.874074\n",
      "Epoch 40, loss: 595.871899\n",
      "Epoch 41, loss: 595.869726\n",
      "Epoch 42, loss: 595.867554\n",
      "Epoch 43, loss: 595.865383\n",
      "Epoch 44, loss: 595.863213\n",
      "Epoch 45, loss: 595.861044\n",
      "Epoch 46, loss: 595.858876\n",
      "Epoch 47, loss: 595.856710\n",
      "Epoch 48, loss: 595.854545\n",
      "Epoch 49, loss: 595.852381\n",
      "Epoch 50, loss: 595.850218\n",
      "Epoch 51, loss: 595.848056\n",
      "Epoch 52, loss: 595.845896\n",
      "Epoch 53, loss: 595.843736\n",
      "Epoch 54, loss: 595.841578\n",
      "Epoch 55, loss: 595.839421\n",
      "Epoch 56, loss: 595.837265\n",
      "Epoch 57, loss: 595.835110\n",
      "Epoch 58, loss: 595.832957\n",
      "Epoch 59, loss: 595.830804\n",
      "Epoch 60, loss: 595.828653\n",
      "Epoch 61, loss: 595.826503\n",
      "Epoch 62, loss: 595.824354\n",
      "Epoch 63, loss: 595.822206\n",
      "Epoch 64, loss: 595.820060\n",
      "Epoch 65, loss: 595.817914\n",
      "Epoch 66, loss: 595.815770\n",
      "Epoch 67, loss: 595.813627\n",
      "Epoch 68, loss: 595.811485\n",
      "Epoch 69, loss: 595.809344\n",
      "Epoch 70, loss: 595.807204\n",
      "Epoch 71, loss: 595.805066\n",
      "Epoch 72, loss: 595.802928\n",
      "Epoch 73, loss: 595.800792\n",
      "Epoch 74, loss: 595.798657\n",
      "Epoch 75, loss: 595.796523\n",
      "Epoch 76, loss: 595.794390\n",
      "Epoch 77, loss: 595.792258\n",
      "Epoch 78, loss: 595.790128\n",
      "Epoch 79, loss: 595.787999\n",
      "Epoch 80, loss: 595.785870\n",
      "Epoch 81, loss: 595.783743\n",
      "Epoch 82, loss: 595.781618\n",
      "Epoch 83, loss: 595.779493\n",
      "Epoch 84, loss: 595.777369\n",
      "Epoch 85, loss: 595.775247\n",
      "Epoch 86, loss: 595.773125\n",
      "Epoch 87, loss: 595.771005\n",
      "Epoch 88, loss: 595.768886\n",
      "Epoch 89, loss: 595.766768\n",
      "Epoch 90, loss: 595.764651\n",
      "Epoch 91, loss: 595.762536\n",
      "Epoch 92, loss: 595.760421\n",
      "Epoch 93, loss: 595.758308\n",
      "Epoch 94, loss: 595.756196\n",
      "Epoch 95, loss: 595.754085\n",
      "Epoch 96, loss: 595.751975\n",
      "Epoch 97, loss: 595.749866\n",
      "Epoch 98, loss: 595.747758\n",
      "Epoch 99, loss: 595.745652\n",
      "1e-05 0.0001 - 0.253\n",
      "Epoch 0, loss: 595.743546\n",
      "Epoch 1, loss: 595.741442\n",
      "Epoch 2, loss: 595.739339\n",
      "Epoch 3, loss: 595.737237\n",
      "Epoch 4, loss: 595.735136\n",
      "Epoch 5, loss: 595.733036\n",
      "Epoch 6, loss: 595.730937\n",
      "Epoch 7, loss: 595.728840\n",
      "Epoch 8, loss: 595.726743\n",
      "Epoch 9, loss: 595.724648\n",
      "Epoch 10, loss: 595.722554\n",
      "Epoch 11, loss: 595.720461\n",
      "Epoch 12, loss: 595.718369\n",
      "Epoch 13, loss: 595.716278\n",
      "Epoch 14, loss: 595.714188\n",
      "Epoch 15, loss: 595.712100\n",
      "Epoch 16, loss: 595.710012\n",
      "Epoch 17, loss: 595.707926\n",
      "Epoch 18, loss: 595.705841\n",
      "Epoch 19, loss: 595.703757\n",
      "Epoch 20, loss: 595.701674\n",
      "Epoch 21, loss: 595.699592\n",
      "Epoch 22, loss: 595.697511\n",
      "Epoch 23, loss: 595.695432\n",
      "Epoch 24, loss: 595.693353\n",
      "Epoch 25, loss: 595.691276\n",
      "Epoch 26, loss: 595.689199\n",
      "Epoch 27, loss: 595.687124\n",
      "Epoch 28, loss: 595.685050\n",
      "Epoch 29, loss: 595.682977\n",
      "Epoch 30, loss: 595.680905\n",
      "Epoch 31, loss: 595.678835\n",
      "Epoch 32, loss: 595.676765\n",
      "Epoch 33, loss: 595.674697\n",
      "Epoch 34, loss: 595.672629\n",
      "Epoch 35, loss: 595.670563\n",
      "Epoch 36, loss: 595.668498\n",
      "Epoch 37, loss: 595.666434\n",
      "Epoch 38, loss: 595.664371\n",
      "Epoch 39, loss: 595.662309\n",
      "Epoch 40, loss: 595.660248\n",
      "Epoch 41, loss: 595.658188\n",
      "Epoch 42, loss: 595.656130\n",
      "Epoch 43, loss: 595.654073\n",
      "Epoch 44, loss: 595.652016\n",
      "Epoch 45, loss: 595.649961\n",
      "Epoch 46, loss: 595.647907\n",
      "Epoch 47, loss: 595.645854\n",
      "Epoch 48, loss: 595.643802\n",
      "Epoch 49, loss: 595.641751\n",
      "Epoch 50, loss: 595.639701\n",
      "Epoch 51, loss: 595.637653\n",
      "Epoch 52, loss: 595.635605\n",
      "Epoch 53, loss: 595.633559\n",
      "Epoch 54, loss: 595.631513\n",
      "Epoch 55, loss: 595.629469\n",
      "Epoch 56, loss: 595.627426\n",
      "Epoch 57, loss: 595.625384\n",
      "Epoch 58, loss: 595.623343\n",
      "Epoch 59, loss: 595.621303\n",
      "Epoch 60, loss: 595.619264\n",
      "Epoch 61, loss: 595.617226\n",
      "Epoch 62, loss: 595.615190\n",
      "Epoch 63, loss: 595.613154\n",
      "Epoch 64, loss: 595.611120\n",
      "Epoch 65, loss: 595.609086\n",
      "Epoch 66, loss: 595.607054\n",
      "Epoch 67, loss: 595.605023\n",
      "Epoch 68, loss: 595.602993\n",
      "Epoch 69, loss: 595.600964\n",
      "Epoch 70, loss: 595.598936\n",
      "Epoch 71, loss: 595.596909\n",
      "Epoch 72, loss: 595.594883\n",
      "Epoch 73, loss: 595.592858\n",
      "Epoch 74, loss: 595.590835\n",
      "Epoch 75, loss: 595.588812\n",
      "Epoch 76, loss: 595.586791\n",
      "Epoch 77, loss: 595.584770\n",
      "Epoch 78, loss: 595.582751\n",
      "Epoch 79, loss: 595.580733\n",
      "Epoch 80, loss: 595.578716\n",
      "Epoch 81, loss: 595.576700\n",
      "Epoch 82, loss: 595.574685\n",
      "Epoch 83, loss: 595.572671\n",
      "Epoch 84, loss: 595.570658\n",
      "Epoch 85, loss: 595.568646\n",
      "Epoch 86, loss: 595.566635\n",
      "Epoch 87, loss: 595.564626\n",
      "Epoch 88, loss: 595.562617\n",
      "Epoch 89, loss: 595.560610\n",
      "Epoch 90, loss: 595.558603\n",
      "Epoch 91, loss: 595.556598\n",
      "Epoch 92, loss: 595.554594\n",
      "Epoch 93, loss: 595.552590\n",
      "Epoch 94, loss: 595.550588\n",
      "Epoch 95, loss: 595.548587\n",
      "Epoch 96, loss: 595.546587\n",
      "Epoch 97, loss: 595.544588\n",
      "Epoch 98, loss: 595.542590\n",
      "Epoch 99, loss: 595.540594\n",
      "1e-05 1e-05 - 0.253\n",
      "Epoch 0, loss: 595.538598\n",
      "Epoch 1, loss: 595.536603\n",
      "Epoch 2, loss: 595.534610\n",
      "Epoch 3, loss: 595.532617\n",
      "Epoch 4, loss: 595.530625\n",
      "Epoch 5, loss: 595.528635\n",
      "Epoch 6, loss: 595.526646\n",
      "Epoch 7, loss: 595.524657\n",
      "Epoch 8, loss: 595.522670\n",
      "Epoch 9, loss: 595.520684\n",
      "Epoch 10, loss: 595.518699\n",
      "Epoch 11, loss: 595.516715\n",
      "Epoch 12, loss: 595.514732\n",
      "Epoch 13, loss: 595.512750\n",
      "Epoch 14, loss: 595.510769\n",
      "Epoch 15, loss: 595.508789\n",
      "Epoch 16, loss: 595.506810\n",
      "Epoch 17, loss: 595.504832\n",
      "Epoch 18, loss: 595.502855\n",
      "Epoch 19, loss: 595.500880\n",
      "Epoch 20, loss: 595.498905\n",
      "Epoch 21, loss: 595.496932\n",
      "Epoch 22, loss: 595.494959\n",
      "Epoch 23, loss: 595.492988\n",
      "Epoch 24, loss: 595.491017\n",
      "Epoch 25, loss: 595.489048\n",
      "Epoch 26, loss: 595.487080\n",
      "Epoch 27, loss: 595.485112\n",
      "Epoch 28, loss: 595.483146\n",
      "Epoch 29, loss: 595.481181\n",
      "Epoch 30, loss: 595.479217\n",
      "Epoch 31, loss: 595.477254\n",
      "Epoch 32, loss: 595.475291\n",
      "Epoch 33, loss: 595.473330\n",
      "Epoch 34, loss: 595.471370\n",
      "Epoch 35, loss: 595.469412\n",
      "Epoch 36, loss: 595.467454\n",
      "Epoch 37, loss: 595.465497\n",
      "Epoch 38, loss: 595.463541\n",
      "Epoch 39, loss: 595.461586\n",
      "Epoch 40, loss: 595.459632\n",
      "Epoch 41, loss: 595.457680\n",
      "Epoch 42, loss: 595.455728\n",
      "Epoch 43, loss: 595.453777\n",
      "Epoch 44, loss: 595.451828\n",
      "Epoch 45, loss: 595.449879\n",
      "Epoch 46, loss: 595.447932\n",
      "Epoch 47, loss: 595.445985\n",
      "Epoch 48, loss: 595.444040\n",
      "Epoch 49, loss: 595.442095\n",
      "Epoch 50, loss: 595.440152\n",
      "Epoch 51, loss: 595.438210\n",
      "Epoch 52, loss: 595.436268\n",
      "Epoch 53, loss: 595.434328\n",
      "Epoch 54, loss: 595.432389\n",
      "Epoch 55, loss: 595.430450\n",
      "Epoch 56, loss: 595.428513\n",
      "Epoch 57, loss: 595.426577\n",
      "Epoch 58, loss: 595.424642\n",
      "Epoch 59, loss: 595.422708\n",
      "Epoch 60, loss: 595.420774\n",
      "Epoch 61, loss: 595.418842\n",
      "Epoch 62, loss: 595.416911\n",
      "Epoch 63, loss: 595.414981\n",
      "Epoch 64, loss: 595.413052\n",
      "Epoch 65, loss: 595.411124\n",
      "Epoch 66, loss: 595.409197\n",
      "Epoch 67, loss: 595.407271\n",
      "Epoch 68, loss: 595.405346\n",
      "Epoch 69, loss: 595.403422\n",
      "Epoch 70, loss: 595.401499\n",
      "Epoch 71, loss: 595.399577\n",
      "Epoch 72, loss: 595.397656\n",
      "Epoch 73, loss: 595.395737\n",
      "Epoch 74, loss: 595.393818\n",
      "Epoch 75, loss: 595.391900\n",
      "Epoch 76, loss: 595.389983\n",
      "Epoch 77, loss: 595.388067\n",
      "Epoch 78, loss: 595.386152\n",
      "Epoch 79, loss: 595.384239\n",
      "Epoch 80, loss: 595.382326\n",
      "Epoch 81, loss: 595.380414\n",
      "Epoch 82, loss: 595.378503\n",
      "Epoch 83, loss: 595.376594\n",
      "Epoch 84, loss: 595.374685\n",
      "Epoch 85, loss: 595.372777\n",
      "Epoch 86, loss: 595.370870\n",
      "Epoch 87, loss: 595.368965\n",
      "Epoch 88, loss: 595.367060\n",
      "Epoch 89, loss: 595.365156\n",
      "Epoch 90, loss: 595.363254\n",
      "Epoch 91, loss: 595.361352\n",
      "Epoch 92, loss: 595.359451\n",
      "Epoch 93, loss: 595.357552\n",
      "Epoch 94, loss: 595.355653\n",
      "Epoch 95, loss: 595.353755\n",
      "Epoch 96, loss: 595.351859\n",
      "Epoch 97, loss: 595.349963\n",
      "Epoch 98, loss: 595.348068\n",
      "Epoch 99, loss: 595.346175\n",
      "1e-05 1e-06 - 0.254\n",
      "{'0,0': 0.204, '0,1': 0.186, '0,2': 0.18, '1,0': 0.236, '1,1': 0.241, '1,2': 0.252, '2,0': 0.253, '2,1': 0.253, '2,2': 0.254}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bestAcc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#{'0,0': 0.187,        \u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# '0,1': 0.186,\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# '0,2': 0.171,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#bestClass.append(classifier.predict(val_X))\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#best_classifier = max(bestClass)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(accMap)\n\u001b[1;32m---> 31\u001b[0m best_val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[43mbestAcc\u001b[49m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# TODO: используя набор данных для валидации найти лучшие гиперпараметры\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest cross-validation accuracy: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m best_val_accuracy)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bestAcc' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "# словарь\n",
    "accMap = {} \n",
    "\n",
    "for i in range(3):\n",
    "    for j in range (3):\n",
    "        classifier.fit(train_X, train_y, epochs=num_epochs, learning_rate=learning_rates[i], batch_size=batch_size, reg=reg_strengths[j])\n",
    "        pred = classifier.predict(val_X)\n",
    "        print(learning_rates[i], reg_strengths[j],\"-\", multiclass_accuracy(pred, val_y));\n",
    "        key = i\n",
    "        accMap[str(i) + \",\" + str(j)] = multiclass_accuracy(pred, val_y)\n",
    "\n",
    "#{'0,0': 0.204,        \n",
    "# '0,1': 0.186,\n",
    "# '0,2': 0.18,\n",
    "# '1,0': 0.236,\n",
    "# '1,1': 0.241,\n",
    "# '1,2': 0.252,\n",
    "# '2,0': 0.253,\n",
    "# '2,1': 0.253,\n",
    "# '2,2': 0.254}\n",
    "\n",
    "#bestClass.append(classifier.predict(val_X))\n",
    "#best_classifier = max(bestClass)\n",
    "print(accMap)\n",
    "best_val_accuracy = max(bestAcc)\n",
    "\n",
    "# TODO: используя набор данных для валидации найти лучшие гиперпараметры\n",
    "print('Best cross-validation accuracy: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 575.729569\n",
      "Epoch 1, loss: 575.638310\n",
      "Epoch 2, loss: 575.549375\n",
      "Epoch 3, loss: 575.462652\n",
      "Epoch 4, loss: 575.378033\n",
      "Epoch 5, loss: 575.295418\n",
      "Epoch 6, loss: 575.214710\n",
      "Epoch 7, loss: 575.135817\n",
      "Epoch 8, loss: 575.058653\n",
      "Epoch 9, loss: 574.983133\n",
      "Epoch 10, loss: 574.909179\n",
      "Epoch 11, loss: 574.836718\n",
      "Epoch 12, loss: 574.765676\n",
      "Epoch 13, loss: 574.695987\n",
      "Epoch 14, loss: 574.627587\n",
      "Epoch 15, loss: 574.560414\n",
      "Epoch 16, loss: 574.494410\n",
      "Epoch 17, loss: 574.429521\n",
      "Epoch 18, loss: 574.365693\n",
      "Epoch 19, loss: 574.302876\n",
      "Epoch 20, loss: 574.241025\n",
      "Epoch 21, loss: 574.180092\n",
      "Epoch 22, loss: 574.120036\n",
      "Epoch 23, loss: 574.060816\n",
      "Epoch 24, loss: 574.002393\n",
      "Epoch 25, loss: 573.944730\n",
      "Epoch 26, loss: 573.887793\n",
      "Epoch 27, loss: 573.831548\n",
      "Epoch 28, loss: 573.775963\n",
      "Epoch 29, loss: 573.721010\n",
      "Epoch 30, loss: 573.666658\n",
      "Epoch 31, loss: 573.612881\n",
      "Epoch 32, loss: 573.559654\n",
      "Epoch 33, loss: 573.506951\n",
      "Epoch 34, loss: 573.454749\n",
      "Epoch 35, loss: 573.403027\n",
      "Epoch 36, loss: 573.351763\n",
      "Epoch 37, loss: 573.300937\n",
      "Epoch 38, loss: 573.250531\n",
      "Epoch 39, loss: 573.200525\n",
      "Epoch 40, loss: 573.150903\n",
      "Epoch 41, loss: 573.101649\n",
      "Epoch 42, loss: 573.052746\n",
      "Epoch 43, loss: 573.004181\n",
      "Epoch 44, loss: 572.955938\n",
      "Epoch 45, loss: 572.908005\n",
      "Epoch 46, loss: 572.860370\n",
      "Epoch 47, loss: 572.813019\n",
      "Epoch 48, loss: 572.765941\n",
      "Epoch 49, loss: 572.719126\n",
      "Epoch 50, loss: 572.672563\n",
      "Epoch 51, loss: 572.626241\n",
      "Epoch 52, loss: 572.580153\n",
      "Epoch 53, loss: 572.534288\n",
      "Epoch 54, loss: 572.488639\n",
      "Epoch 55, loss: 572.443196\n",
      "Epoch 56, loss: 572.397953\n",
      "Epoch 57, loss: 572.352902\n",
      "Epoch 58, loss: 572.308036\n",
      "Epoch 59, loss: 572.263348\n",
      "Epoch 60, loss: 572.218833\n",
      "Epoch 61, loss: 572.174484\n",
      "Epoch 62, loss: 572.130295\n",
      "Epoch 63, loss: 572.086261\n",
      "Epoch 64, loss: 572.042376\n",
      "Epoch 65, loss: 571.998637\n",
      "Epoch 66, loss: 571.955037\n",
      "Epoch 67, loss: 571.911574\n",
      "Epoch 68, loss: 571.868241\n",
      "Epoch 69, loss: 571.825036\n",
      "Epoch 70, loss: 571.781955\n",
      "Epoch 71, loss: 571.738993\n",
      "Epoch 72, loss: 571.696148\n",
      "Epoch 73, loss: 571.653415\n",
      "Epoch 74, loss: 571.610793\n",
      "Epoch 75, loss: 571.568277\n",
      "Epoch 76, loss: 571.525865\n",
      "Epoch 77, loss: 571.483554\n",
      "Epoch 78, loss: 571.441342\n",
      "Epoch 79, loss: 571.399226\n",
      "Epoch 80, loss: 571.357204\n",
      "Epoch 81, loss: 571.315273\n",
      "Epoch 82, loss: 571.273431\n",
      "Epoch 83, loss: 571.231677\n",
      "Epoch 84, loss: 571.190008\n",
      "Epoch 85, loss: 571.148422\n",
      "Epoch 86, loss: 571.106918\n",
      "Epoch 87, loss: 571.065494\n",
      "Epoch 88, loss: 571.024149\n",
      "Epoch 89, loss: 570.982880\n",
      "Epoch 90, loss: 570.941686\n",
      "Epoch 91, loss: 570.900566\n",
      "Epoch 92, loss: 570.859519\n",
      "Epoch 93, loss: 570.818543\n",
      "Epoch 94, loss: 570.777637\n",
      "Epoch 95, loss: 570.736800\n",
      "Epoch 96, loss: 570.696030\n",
      "Epoch 97, loss: 570.655328\n",
      "Epoch 98, loss: 570.614691\n",
      "Epoch 99, loss: 570.574119\n",
      "Epoch 100, loss: 570.533610\n",
      "Epoch 101, loss: 570.493165\n",
      "Epoch 102, loss: 570.452781\n",
      "Epoch 103, loss: 570.412459\n",
      "Epoch 104, loss: 570.372197\n",
      "Epoch 105, loss: 570.331995\n",
      "Epoch 106, loss: 570.291851\n",
      "Epoch 107, loss: 570.251767\n",
      "Epoch 108, loss: 570.211739\n",
      "Epoch 109, loss: 570.171769\n",
      "Epoch 110, loss: 570.131855\n",
      "Epoch 111, loss: 570.091997\n",
      "Epoch 112, loss: 570.052194\n",
      "Epoch 113, loss: 570.012446\n",
      "Epoch 114, loss: 569.972752\n",
      "Epoch 115, loss: 569.933112\n",
      "Epoch 116, loss: 569.893526\n",
      "Epoch 117, loss: 569.853992\n",
      "Epoch 118, loss: 569.814510\n",
      "Epoch 119, loss: 569.775081\n",
      "Epoch 120, loss: 569.735703\n",
      "Epoch 121, loss: 569.696376\n",
      "Epoch 122, loss: 569.657100\n",
      "Epoch 123, loss: 569.617875\n",
      "Epoch 124, loss: 569.578699\n",
      "Epoch 125, loss: 569.539574\n",
      "Epoch 126, loss: 569.500498\n",
      "Epoch 127, loss: 569.461471\n",
      "Epoch 128, loss: 569.422493\n",
      "Epoch 129, loss: 569.383563\n",
      "Epoch 130, loss: 569.344682\n",
      "Epoch 131, loss: 569.305849\n",
      "Epoch 132, loss: 569.267063\n",
      "Epoch 133, loss: 569.228325\n",
      "Epoch 134, loss: 569.189635\n",
      "Epoch 135, loss: 569.150991\n",
      "Epoch 136, loss: 569.112394\n",
      "Epoch 137, loss: 569.073844\n",
      "Epoch 138, loss: 569.035340\n",
      "Epoch 139, loss: 568.996882\n",
      "Epoch 140, loss: 568.958470\n",
      "Epoch 141, loss: 568.920104\n",
      "Epoch 142, loss: 568.881784\n",
      "Epoch 143, loss: 568.843509\n",
      "Epoch 144, loss: 568.805279\n",
      "Epoch 145, loss: 568.767094\n",
      "Epoch 146, loss: 568.728953\n",
      "Epoch 147, loss: 568.690858\n",
      "Epoch 148, loss: 568.652807\n",
      "Epoch 149, loss: 568.614800\n",
      "Epoch 150, loss: 568.576838\n",
      "Epoch 151, loss: 568.538920\n",
      "Epoch 152, loss: 568.501045\n",
      "Epoch 153, loss: 568.463214\n",
      "Epoch 154, loss: 568.425427\n",
      "Epoch 155, loss: 568.387683\n",
      "Epoch 156, loss: 568.349983\n",
      "Epoch 157, loss: 568.312326\n",
      "Epoch 158, loss: 568.274711\n",
      "Epoch 159, loss: 568.237140\n",
      "Epoch 160, loss: 568.199611\n",
      "Epoch 161, loss: 568.162126\n",
      "Epoch 162, loss: 568.124682\n",
      "Epoch 163, loss: 568.087281\n",
      "Epoch 164, loss: 568.049922\n",
      "Epoch 165, loss: 568.012606\n",
      "Epoch 166, loss: 567.975331\n",
      "Epoch 167, loss: 567.938099\n",
      "Epoch 168, loss: 567.900908\n",
      "Epoch 169, loss: 567.863759\n",
      "Epoch 170, loss: 567.826652\n",
      "Epoch 171, loss: 567.789586\n",
      "Epoch 172, loss: 567.752562\n",
      "Epoch 173, loss: 567.715578\n",
      "Epoch 174, loss: 567.678636\n",
      "Epoch 175, loss: 567.641735\n",
      "Epoch 176, loss: 567.604875\n",
      "Epoch 177, loss: 567.568056\n",
      "Epoch 178, loss: 567.531278\n",
      "Epoch 179, loss: 567.494540\n",
      "Epoch 180, loss: 567.457843\n",
      "Epoch 181, loss: 567.421187\n",
      "Epoch 182, loss: 567.384570\n",
      "Epoch 183, loss: 567.347994\n",
      "Epoch 184, loss: 567.311459\n",
      "Epoch 185, loss: 567.274963\n",
      "Epoch 186, loss: 567.238507\n",
      "Epoch 187, loss: 567.202092\n",
      "Epoch 188, loss: 567.165716\n",
      "Epoch 189, loss: 567.129380\n",
      "Epoch 190, loss: 567.093083\n",
      "Epoch 191, loss: 567.056826\n",
      "Epoch 192, loss: 567.020609\n",
      "Epoch 193, loss: 566.984431\n",
      "Epoch 194, loss: 566.948292\n",
      "Epoch 195, loss: 566.912192\n",
      "Epoch 196, loss: 566.876132\n",
      "Epoch 197, loss: 566.840110\n",
      "Epoch 198, loss: 566.804128\n",
      "Epoch 199, loss: 566.768184\n",
      "Epoch 200, loss: 566.732279\n",
      "Epoch 201, loss: 566.696413\n",
      "Epoch 202, loss: 566.660585\n",
      "Epoch 203, loss: 566.624796\n",
      "Epoch 204, loss: 566.589046\n",
      "Epoch 205, loss: 566.553334\n",
      "Epoch 206, loss: 566.517660\n",
      "Epoch 207, loss: 566.482024\n",
      "Epoch 208, loss: 566.446426\n",
      "Epoch 209, loss: 566.410867\n",
      "Epoch 210, loss: 566.375345\n",
      "Epoch 211, loss: 566.339862\n",
      "Epoch 212, loss: 566.304416\n",
      "Epoch 213, loss: 566.269008\n",
      "Epoch 214, loss: 566.233637\n",
      "Epoch 215, loss: 566.198304\n",
      "Epoch 216, loss: 566.163009\n",
      "Epoch 217, loss: 566.127751\n",
      "Epoch 218, loss: 566.092530\n",
      "Epoch 219, loss: 566.057347\n",
      "Epoch 220, loss: 566.022200\n",
      "Epoch 221, loss: 565.987091\n",
      "Epoch 222, loss: 565.952019\n",
      "Epoch 223, loss: 565.916984\n",
      "Epoch 224, loss: 565.881986\n",
      "Epoch 225, loss: 565.847025\n",
      "Epoch 226, loss: 565.812100\n",
      "Epoch 227, loss: 565.777212\n",
      "Epoch 228, loss: 565.742360\n",
      "Epoch 229, loss: 565.707545\n",
      "Epoch 230, loss: 565.672767\n",
      "Epoch 231, loss: 565.638025\n",
      "Epoch 232, loss: 565.603319\n",
      "Epoch 233, loss: 565.568649\n",
      "Epoch 234, loss: 565.534016\n",
      "Epoch 235, loss: 565.499418\n",
      "Epoch 236, loss: 565.464857\n",
      "Epoch 237, loss: 565.430332\n",
      "Epoch 238, loss: 565.395842\n",
      "Epoch 239, loss: 565.361388\n",
      "Epoch 240, loss: 565.326970\n",
      "Epoch 241, loss: 565.292588\n",
      "Epoch 242, loss: 565.258241\n",
      "Epoch 243, loss: 565.223929\n",
      "Epoch 244, loss: 565.189653\n",
      "Epoch 245, loss: 565.155413\n",
      "Epoch 246, loss: 565.121207\n",
      "Epoch 247, loss: 565.087037\n",
      "Epoch 248, loss: 565.052902\n",
      "Epoch 249, loss: 565.018803\n",
      "Epoch 250, loss: 564.984738\n",
      "Epoch 251, loss: 564.950708\n",
      "Epoch 252, loss: 564.916713\n",
      "Epoch 253, loss: 564.882753\n",
      "Epoch 254, loss: 564.848827\n",
      "Epoch 255, loss: 564.814936\n",
      "Epoch 256, loss: 564.781080\n",
      "Epoch 257, loss: 564.747259\n",
      "Epoch 258, loss: 564.713471\n",
      "Epoch 259, loss: 564.679719\n",
      "Epoch 260, loss: 564.646000\n",
      "Epoch 261, loss: 564.612316\n",
      "Epoch 262, loss: 564.578666\n",
      "Epoch 263, loss: 564.545050\n",
      "Epoch 264, loss: 564.511468\n",
      "Epoch 265, loss: 564.477921\n",
      "Epoch 266, loss: 564.444407\n",
      "Epoch 267, loss: 564.410927\n",
      "Epoch 268, loss: 564.377480\n",
      "Epoch 269, loss: 564.344068\n",
      "Epoch 270, loss: 564.310689\n",
      "Epoch 271, loss: 564.277344\n",
      "Epoch 272, loss: 564.244032\n",
      "Epoch 273, loss: 564.210754\n",
      "Epoch 274, loss: 564.177509\n",
      "Epoch 275, loss: 564.144298\n",
      "Epoch 276, loss: 564.111120\n",
      "Epoch 277, loss: 564.077975\n",
      "Epoch 278, loss: 564.044863\n",
      "Epoch 279, loss: 564.011785\n",
      "Epoch 280, loss: 563.978739\n",
      "Epoch 281, loss: 563.945726\n",
      "Epoch 282, loss: 563.912747\n",
      "Epoch 283, loss: 563.879800\n",
      "Epoch 284, loss: 563.846885\n",
      "Epoch 285, loss: 563.814004\n",
      "Epoch 286, loss: 563.781155\n",
      "Epoch 287, loss: 563.748339\n",
      "Epoch 288, loss: 563.715555\n",
      "Epoch 289, loss: 563.682804\n",
      "Epoch 290, loss: 563.650085\n",
      "Epoch 291, loss: 563.617399\n",
      "Epoch 292, loss: 563.584744\n",
      "Epoch 293, loss: 563.552122\n",
      "Epoch 294, loss: 563.519532\n",
      "Epoch 295, loss: 563.486975\n",
      "Epoch 296, loss: 563.454449\n",
      "Epoch 297, loss: 563.421955\n",
      "Epoch 298, loss: 563.389493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299, loss: 563.357063\n",
      "Linear softmax classifier test set accuracy: 0.223000\n"
     ]
    }
   ],
   "source": [
    "best_classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "best_classifier.fit(test_X, test_y, epochs=300, learning_rate=1e-5, batch_size=250, reg=1e-6)\n",
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
